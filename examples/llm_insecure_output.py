"""
Insecure Output Handling Example - OWASP LLM02:2025

This code demonstrates vulnerabilities where LLM output is
trusted and used without proper validation or sanitization.

Expected Findings:
- CRITICAL: LLM output executed without validation (confidence: 1.0)
- HIGH: No output sanitization before rendering (confidence: 0.95)
- HIGH: Unsanitized output passed to shell (confidence: 0.95)
- MEDIUM: LLM output used in database queries (confidence: 0.85)

Expected Verdict: ðŸš« BLOCK

Attack Vector:
    1. Attacker crafts prompt to make LLM output malicious code
    2. LLM output: "import os; os.system('rm -rf /')"
    3. Application executes the output directly

    Or via injection through training data:
    1. Poisoned training data includes malicious patterns
    2. LLM generates harmful output
    3. Application trusts and executes it

Remediation:
    1. Never execute LLM output directly
    2. Sanitize output before rendering (escape HTML, JS)
    3. Validate output format matches expected schema
    4. Use allowlists for any LLM-generated commands
    5. Apply principle of least privilege to LLM actions
"""

import subprocess
from typing import Any

import openai


class LLMOutputHandler:
    """Demonstrates insecure handling of LLM outputs."""

    def __init__(self, client: Any = None):
        self.client = client or openai.OpenAI()

    def execute_code_vulnerable(self, user_request: str) -> Any:
        """
        VULNERABLE: Executes code generated by LLM without validation.

        LLM02:2025 - The LLM output is directly executed, allowing
        arbitrary code execution if the LLM is compromised or manipulated.
        """
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Generate Python code for the request."},
                {"role": "user", "content": user_request},
            ],
        )

        code = response.choices[0].message.content
        exec(code)  # CRITICAL: Executes untrusted LLM output
        return code

    def run_shell_command_vulnerable(self, user_request: str) -> str:
        """
        VULNERABLE: Runs shell command generated by LLM.

        Attacker can manipulate LLM to generate malicious commands.
        """
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": "Generate a shell command for the request.",
                },
                {"role": "user", "content": user_request},
            ],
        )

        command = response.choices[0].message.content
        result = subprocess.run(command, shell=True, capture_output=True)  # CRITICAL
        return result.stdout.decode()

    def render_html_vulnerable(self, user_request: str) -> str:
        """
        VULNERABLE: Renders LLM-generated HTML without sanitization.

        Can lead to XSS if LLM output contains malicious scripts.
        """
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Generate HTML for the request."},
                {"role": "user", "content": user_request},
            ],
        )

        html = response.choices[0].message.content
        return f"<div class='llm-output'>{html}</div>"  # XSS risk

    def query_database_vulnerable(self, user_request: str, db_cursor: Any) -> list:
        """
        VULNERABLE: Uses LLM-generated SQL directly.

        LLM could generate malicious queries if manipulated.
        """
        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "Generate a SQL query for the request."},
                {"role": "user", "content": user_request},
            ],
        )

        query = response.choices[0].message.content
        db_cursor.execute(query)  # SQL injection via LLM
        return db_cursor.fetchall()


# Example of SECURE implementation
class SecureLLMOutputHandler:
    """Demonstrates secure handling of LLM outputs."""

    def __init__(self, client: Any = None):
        self.client = client or openai.OpenAI()
        self.allowed_operations = {"list_files", "read_file", "calculate"}

    def execute_operation_secure(self, user_request: str) -> Any:
        """
        SECURE: Validates LLM output against allowlist.
        """
        import json

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": """Return JSON with format: 
                {"operation": "list_files|read_file|calculate", "args": {...}}""",
                },
                {"role": "user", "content": user_request},
            ],
        )

        # Parse and validate
        try:
            output = json.loads(response.choices[0].message.content)
        except json.JSONDecodeError:
            raise ValueError("LLM returned invalid JSON")

        # Validate operation against allowlist
        if output.get("operation") not in self.allowed_operations:
            raise ValueError(f"Operation not allowed: {output.get('operation')}")

        # Execute only pre-defined, safe operations
        return self._execute_safe_operation(output["operation"], output.get("args", {}))

    def _execute_safe_operation(self, operation: str, args: dict) -> Any:
        """Execute pre-validated operations only."""
        if operation == "list_files":
            import os

            return os.listdir(args.get("path", "."))
        elif operation == "calculate":
            # Safe math evaluation only
            expr = args.get("expression", "0")
            if not all(c in "0123456789+-*/() " for c in expr):
                raise ValueError("Invalid expression")
            return eval(expr)
        return None


if __name__ == "__main__":
    print("LLM02:2025 - Insecure Output Handling Examples")
    print("=" * 50)
    print("This file demonstrates vulnerable patterns where")
    print("LLM output is trusted without proper validation.")
